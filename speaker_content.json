"Speaker A:  Dr. Andrew Ning just did a talk at Sequoia and is all about agents and he is incredibly  bullish on agents. He said things like GPT-3.5 powering agents can actually reason to the level  of GPT-4 and a lot of other really interesting tidbits. So we're gonna watch his talk together  and I'm gonna walk you through step-by-step what he's saying and why it's so important.  I am incredibly bullish on agents myself, that's why I make so many videos about them,  and I truly believe the future of artificial intelligence is going to be agentic. So first,  who is Dr. Andrew Ning? He is a computer scientist. He was the co-founder and head of Google Brain,  the former chief scientist of Baidu, and a leading mind in artificial intelligence. He went to UC  Berkeley, MIT, and Carnegie Mellon. So smart, smart dude. And he co-founded this company Coursera  where you can learn a ton about computer science, about math, a bunch of different topics,  absolutely free. And so what he's doing is truly incredible. And so when he talks about AI,  you should listen. So let's get to this talk. This is at Sequoia and if you're not familiar  with Sequoia, they are one of the most legendary Silicon Valley venture capital firms ever. Now,  here's an interesting stat about Sequoia that just shows how incredible they are at picking  technological winners. Their portfolio of companies represents more than 25% of today's  total value of the NASDAQ. So the total value of all the companies that are listed on the NASDAQ,  25% of that market capitalization are companies that are owned or have been owned or invested in  by Sequoia. Incredible stat. Let's look at some of their companies. Reddit, Instacart, DoorDash,  Airbnb, a little company called Apple, Block, Snowflake, Vanta, Zoom, Stripe, WhatsApp, Okta,  Instagram. This list is absolutely absurd. All right, enough of the preface. Let me get into \nSpeaker B:  the talk itself. So AI agents, you know, today, the way most of us use large language models is  like this with a non-agentic workflow, where you type a prompt and generate an answer. And that's  a bit like if you ask a person to write an essay on a topic and I say, please sit down to the  keyboard and just type the essay from start to finish without ever using backspace. Um, and  despite how hard this is, LMs do it remarkably well. In contrast with an agentic workflow,  this is what it may look like. Have an AI, have an LM say, write an essay outline. Do you need  to do any web research? If so, let's do that. Then write the first draft and then read your  own first draft and think about what parts need revision. And then revise your draft and you go  on and on. And so this workflow is much more iterative, where you may have the LM do some  thinking, um, and then revise this article and then do some more thinking and iterate this \nSpeaker A:  through a number of times. So I want to pause it there and talk about this, because this is the  best explanation for why agents are so powerful. I've heard a lot of people say, well, agents are  just LLMs, right? And yeah, technically that's true. But the power of an agentic workflow is the  fact that you can have multiple agents, all with different roles, different backgrounds, different  personas, different tools, working together and iterating. That's the important word, iterating  on a task. So in this example, he said, okay, write an essay and yeah, an LLM can do that. And  usually it's pretty darn good. But now let's say you have one agent who is the writer, another  agent who is the reviewer, another for the spell checker, another for the grammar checker, another  for the fact checker, and they're all working together and they iterate over and over again,  passing the essay back and forth, making sure that it finally ends up to be the best possible  outcome. And so this is how humans work. Humans, as he said, do not just do everything in one take  without thinking through and planning. We plan, we iterate, and then we find the best solution. \nSpeaker B:  So let's keep listening. What not many people appreciate is this delivers remarkably better  results. I've actually really surprised myself working with these agent workflows,  how well they work. I'm going to do one case study. My team analyzed some data  using a coding benchmark called the Human Eval Benchmark, released by OpenAI a few years ago.  But this has coding problems like, given the non-empty list of integers, return the sum of  all the odd elements or uneven positions. And it turns out the answer is, you know, code snippet  like that. So today, a lot of us will use zero-shot prompting, meaning we tell the AI, write the code  and have it run on the first pass. Like, who codes like that? No human codes like that. We just type  out the code and run it. Maybe you do. I can't do that. So it turns out that if you use GPT 3.5,  zero-shot prompting, it gets it 48% right. GPT 4, way better, 67% right. But if you take an  agentic workflow and wrap it around GPT 3.5, say, it actually does better than even GPT 4.  And if you were to wrap this type of workflow around GPT 4, you know, it also does very well. \nSpeaker A:  All right, let's pause here and think about what he just said. Over here, we have the zero-shot,  which basically means you're simply telling the large language model, do this thing, not giving  it any examples, not giving it any chance to think or to iterate or any fancy prompting. Just do this  thing. And it got the human eval benchmark 48% correct. Then GPT 4, 67%, which is, you know,  a huge improvement. And we're going to continue to see improvement when GPT 5 comes out and so on.  However, look at this. GPT 3.5 wrapped in an agentic workflow, any of these, all perform  better than the zero-shot GPT 4 using only GPT 3.5. And this LBD plus reflection, it's actually  nearly 100%. It's over 95%. Then, of course, if we wrap GPT 4 in the agentic workflow, MetaGPT,  for example, we all know about, it performs incredibly well across the board. And agent  coder kind of at the top here. So it's really just showing the power of agentic workflows. \nSpeaker B:  And you notice that GPT 3.5 with an agentic workflow actually outperforms GPT 4. And I think  this has, and this means that this has significant consequences, I think, how we all approach  building applications. So agents is a term that's been tossed around a lot. There's a lot of  consultant reports, how about agents, the future of AI, blah, blah, blah. I want to be a bit  concrete and share with you the broad design patterns I'm seeing in agents. It's a very  messy, chaotic space. Tons of research, tons of open source. There's a lot going on. But I try  to categorize a bit more concretely what's going on in agents. Reflection is a tool that I think  many of us should just use. It just works. Tool use, I think it's more widely appreciated, but \nSpeaker A:  actually works pretty well. I think of these as pretty robust technologies. All right, let's stop  there and talk about what these things are. So reflection is as obvious as it sounds. You are  literally saying to the large language model, reflect on the output you just gave me, find a  way to improve it, then return another result or just return the improvements. So very straightforward.  And it seems so obvious, but this actually causes large language models to perform a lot better.  And then we have tool use. And we learned all about tool use with projects like Autogen and  Crew AI. Tool use just means that you can give them tools to use. You can custom code tools. It's  like function calling. So you could say, OK, I want a web scraping tool and I want an SEC lookup tool  so you can get stock information about ticker symbols. You can even plug in complex math  libraries to it. I mean, the possibilities are literally endless. So you can give a bunch of  tools that the large language model didn't previously have. You just describe what the  tool does and the large language model can actually choose when to use the tool. It's really cool. \nSpeaker B:  Use them. I can almost always get them to work well. Planning and multi-agent collaboration, I  think it is more emerging when I use them. Sometimes my mind is blown for how well they work.  But at least at this moment in time, I don't feel like I can always get them to work reliably.  So let me walk through these four design patterns in a few seconds. \nSpeaker A:  All right. So he's going to walk through it, but I just want to touch on what planning and  multi-agent collaboration is. So planning, we're basically saying giving the large language model  the ability to think more slowly, to plan steps. And that's usually, by the way, why in all of my  LLM tests I say, explain your reasoning step by step, because that kind of forces them to plan  and to think through each step, which usually produces better results. And then multi-agent  collaboration, that is autogen and crew AI. That is a very emergent technology. I am extremely  bullish on it. It is sometimes difficult to get the agents to behave like you need them to,  but with enough QA and enough testing and iteration, you usually can, and the results  are phenomenal. And not only do you get the benefit of having the large language model  essentially reflect with different personalities or different roles, but you can actually have  different models powering different agents. And so you're getting the benefit of the reflection  based on the quality of each model. So you're basically getting really differing opinions  as these agents are working together. So let's keep listening. \nSpeaker B:  And if some of you go back and yourself or ask your engineers to use these, I think you get  a positivity boost quite quickly. So reflection, here's an example. Let's say I ask a system,  please write code for me for a given task. Then we have a coder agent, just an LLM that you  prompt to write code, to say, you know, def do task, write a function like that.  Um, an example of self-reflection would be if you then prompt the LLM with something like this,  here's code intended for a task, and just give it back the exact same code that you just generated.  And then say, check the code carefully for correctness, sound efficiency,  good construction, just write a prompt like that. It turns out the same LLM that you prompted to  write the code may be able to spot problems like this, bug in line five, may fix it by blah, blah,  blah. And if you now take his own feedback and give it to it and re-prompt it,  it may come up with a version two of the code that could well work better than the first version.  Not guaranteed, but it works, you know, often enough for this to be worth trying for a lot of \nSpeaker A:  applicants. So what you usually see me doing in my LLM test videos is, for example, let's say I say,  write the game Snake in Python, and it gives me the game Snake. It's, that is zero shot. I'm just  saying, write it all out in one go. Then I take it, I put it in my VS code, I play it, I get the  error, or I look for any bugs, and then I paste that back in to the LLM to fix. Now that's  essentially me acting as an agent, and what we can do is use an agent to automate me. So basically,  look at the code, look for any potential errors, and even agents that can run the code, get the  error, and pass it back into the LLM. Now it's completely automated coding. \nSpeaker A:  single coder agent, you can have two agents,  where one is a coder agent and the second is a critic agent.  And these could be the same base LLM model,  but that you prompt in different ways.  We say one, you're an expert coder, write code.  The other one say, you're an expert code reviewer, so review this code.  And this type of workflow is actually pretty easy to implement.  I think it's such a very general purpose technology for a lot of workflows.  This would give you a significant boost in,  in the performance of LLMs.  Um, the second design pattern is tool use.  Many of you will already have seen, you know,  LLM-based systems, uh, uh, using tools.  On the left is a screenshot from, um, uh,  Copilot, on the right is something that I kind of extracted from, uh, GPT-4.  But, you know, LLMs today, uh,  if you ask it, what's the best coffee maker,  the new web search, for some problems,  LLMs will generate code and run codes.  Um, and it turns out that there are a lot of  different tools that many different people are using for analysis,  for gathering information,  for taking actions, for personal productivity.  Um, it turns out a lot of the early work in tool use  turned out to be in the computer vision community.  Because before large language models,  LLMs, you know, they couldn't do anything with images.  So the only option was to have the LLM generate  a function call that could manipulate an image,  like generate an image or do object detection or whatever.  So if you actually look at literature,  it's been interesting how much of the work, um,  in tool use seems like it originated from vision.  Because LLMs were blind to images before,  you know, GPT-4v and, and, and Lava and so on.  Um, so that's tool use and it-  All right. So tool use,  incredibly, incredibly important.  Because you're basically giving the large language model code to use.  It is hard-coded code.  So you always know the results.  It's not another large language model  that might produce something a little different each time.  This is hard-coded and always is going to produce the same output.  So these tools are very valuable.  And the cool thing about tools is we don't have to rewrite them, right?  We don't have to write them from scratch. \nSpeaker B:  These are tools that programmers already tapped to use in their code.  So whether it's external libraries, API calls,  all of these things can now be used by large language models.  And that is really exciting.  We're not going to have to rewrite all of this tooling.  And then planning, you know,  for those of you that have not yet played a lot with planning algorithms,  I- I feel like a lot of people talk about the chat GPT moment where you're,  wow, never seen anything like this.  I think if you've not used planning algorithms,  many people will have a kind of a AI agent, wow.  I couldn't imagine an AI agent doing this.  So I've run live demos where something failed,  and the AI agent rerouted around the failure.  So I've actually had quite a few of those moments where, wow.  You know, I can't believe my AI system just did that autonomously.  But, um, one example that I adapted from a Hugging GPT paper, you know,  you say, please generate an image where the girls read- where a girl-  And by the way, I made a video about Hugging GPT.  It is an amazing paper. \nSpeaker A:  I'll link that in the description below.  Girls reading a book, and the pose is the same as a boy in the image,  example.jpeg, and please describe the new image with the boy.  So give an example like this.  Um, today with AI agents,  you can kind of decide,  first thing I need to do is determine the pose of the boy, um,  then, you know, find the right model,  maybe on Hugging Face to extract the pose.  Then next, you need to find a pose-to-image model to synthesize a picture of a,  of a girl of- as following the instructions,  then use, uh, image-to-text,  so- and then finally use text-to-speech.  And today, we actually have agents that,  I don't want to say they work reliably.  You know, they're kind of finicky.  They don't always work, but when it works,  it's actually pretty amazing.  But with agentic groups,  sometimes you can recover from earlier failures as well.  So-  Yeah, and that's a really important point.  Agents are a little bit finicky,  but since you can iterate and the agents can usually recover from their issues,  that makes them a lot more powerful.  And as we continue to evolve agents,  as we get better agentic models,  better tooling, better frameworks like Crew AI and Autogen, \nSpeaker B:  all of these kind of finicky aspects of agents are going to start to get reduced tremendously.  I find myself already using research agents in some of my work,  where I'll run a piece of research,  but I don't feel like, you know,  Googling myself and spend a long time.  I should send to the research agent,  come back in a few minutes and see what it's come up with.  And- and it sometimes works,  sometimes it doesn't, right?  But that's already a part of my personal workflow.  The final design pattern, multi-agent collaboration.  This is one of those funny things,  but, uh, um, it works much better than you might think, uh, uh.  But on the left is a screenshot from a paper called, um, Chat Dev.  I made a video about this.  It'll be in the description below as well.  Uh, which is completely open,  which is actually open source.  Many of you saw the, you know,  flashy social media announcements of demo of a Devin.  Uh, uh, Chat Dev is actually open source.  It runs on my laptop.  And what Chat Dev does is example of a multi-agent system,  where you prompt one LLM to sometimes act like the CEO of a software engine company,  sometimes act like a designer,  sometimes act like a product manager,  sometimes act like a tester. \nSpeaker A:  And this flock of agents that you build by prompting an LLM to tell them,  you are now a CEO,  you are now a software engineer.  They collaborate, have an extended conversation,  so that if you tell it,  please develop a game,  develop a GoMoji game.  They'll actually spend, you know,  a few minutes writing code,  testing it, uh, iterating,  and then generate, uh, like surprisingly complex programs.  Doesn't always work.  You know, I've used it.  Sometimes it doesn't work.  Sometimes it's amazing.  But this technology is really, um, getting better.  And, and just one of the design pattern,  it turns out that multi-agent debate,  where you have different agents,  you know, for example, it could be,  have ChatGPT and Gemini debate each other.  That actually results in, uh, uh, better performance as well.  All right. So he said the important part right there.  When you have different agents,  and each of them are powered by different models,  maybe even fine-tuned models,  fine-tuned specifically for their task and their role,  you get really good performance.  And that is exactly what a project like Crew AI, \nSpeaker B:  like AutoGen is made for.  So getting multiple simulated agents work  together has been a powerful design pattern as well.  Um, so just to summarize,  I think these are the,  these are the, the, the, uh, patterns I've seen.  And I think that if we were to, um,  use these, uh, uh, patterns, you know, in our work,  a lot of us can get a productivity boost quite quickly.  And I think that, uh,  agentic reasoning design patterns are gonna be important.  Uh, this is my last slide.  I expect that the set of tasks AI could do will expand  dramatically this year, uh, because of agentic workflows.  And one thing that is actually difficult for people to get used to is,  when we prompt an LLM,  we want to response right away.  Um, in fact, a decade ago when I was, you know,  having discussions there at, at, \nSpeaker A:  at Google on, um, called a big box, uh, search.  We type a long prompt.  One of the reasons, you know,  I failed to push successfully for that was because when you do a web search,  you want a response back in half a second, right?  That's just human nature.  We like that instant grab, instant feedback.  But for a lot of the agent workflows, um,  I think we'll need to learn to dedicate a task in  AI agents and patiently wait minutes,  maybe even hours, uh, to, for response.  But just like, I've seen a lot of novice managers  delegate something to someone and they check in five minutes later, right?  And that's not productive.  Um, I think we need to, it's really difficult.  We need to do that with some of our AI agents as well.  All right. So this is actually a point which  I want to pose a different way of thinking about it.  Think about Grok, Grok G-R-O-Q.  You get 500, 700,  850 tokens per second with Grok, with their architecture.  And all of a sudden, the agents, which, you know,  you usually expect them to take a few minutes to do a semi-complex task, \nSpeaker B:  all the way up to 10, 15, 20 minutes, depending on what the task is.  A lot of the time in that task completion is the inference running.  That is assuming you're getting, you know,  10, 15, 20 tokens per second with open AI.  But if you're able to get 800 tokens per second,  it's essentially instant.  And a lot of people, when they first saw Grok, they thought, \nSpeaker A:  well, what's the point of 800 tokens per second  because humans can't read that fast.  This is the best use case for that.  Agents using hyper inference speed and reading each other's responses  is the best way to leverage that really fast inference speed.  Humans don't actually need to read it.  So this is a perfect example.  So if all of a sudden that part of your agent workflow is extremely fast,  and then let's say we get an embeddings model to be that fast,  all of a sudden, the slowest part of the entire agent workflow  is going to be searching the web or hitting a third-party API.  It's no longer going to be the inference and the embeddings.  And that is really exciting.  Let's keep watching the end.  And then one other important trend, fast token generation is important  because with these agentic workflows, we're iterating over and over.  So the LM is generating tokens for the LM to read.  And I think that generating more tokens really quickly  from even a slightly lower quality LM might give good results  compared to slower tokens from a better LM. \nSpeaker B:  Maybe.  It's a little bit controversial because it  may let you go around this loop a lot more times,  kind of like the results I showed with GPT-3 and an agent  architecture on the first slide.  And candidly, I'm really looking forward to Cloud 5 and- Cloud 4 and GPT-5  and Gemini 2.0 and all these other wonderful models  that many people are building.  And part of me feels like if you're looking forward  to running your thing on GPT-5 zero-shot,  you may be able to get closer to that level of performance  on some applications than you might think with agenting reasoning,  but on an early model.  I think this is an important trend.  And honestly, the path to AGI feels like a journey rather than a destination.  But I think this type of agent workflows  could help us take a small step forward on this very long journey.  Thank you.  OK, so he said a lot of important things at the end there.  One thing he said is if you're already looking forward to GPT-5, Cloud 4,  basically the next generation of the cutting edge models, \nSpeaker A:  you might be able to achieve much of their performance  by just using this agentic workflow and current models.  That is really impressive.  And he also said fast token generation is going to be critical.  And I couldn't agree more.  I'm actually playing with Grok right now, doing a lot of testing,  trying to find the best script to essentially give it a prompt,  let it iterate a bunch of times on it using an open source model like Mixtral,  and getting the best possible performance out of it, GPT-4 level performance.  And I'm still playing around with it.  There's going to be a video for that coming.  I'm really excited about that one.  But it just shows just the fact that an agent can do reflection, planning,  iteration, these things improve output tremendously. \n"